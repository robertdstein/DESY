\documentclass{article}
\usepackage{graphicx}
\graphicspath{ {graphs/} }
\usepackage{geometry}
\geometry{a4paper, portrait, margin=1.0in}
\begin{document}
\title{Reconstruction of Charge Number of Heavy Cosmic Rays using Cherenkov Light}
\author{Robert Stein}
\maketitle
\section{Introduction}
There are numerous Telescope Arrays which image the Cherenkov Light emitted by Cosmic Rays (CRs) in the atmosphere, all relying on Hillas Analysis for event reconstruction. Hillas Analysis extracts parameters from each of the camera images in order to reconstruct the events, but heavy atmospheric blurring means that charge resolution is very poor. For Iron Nucleus events, we would expect to reconstruct $Z \approx 26 \pm 5 $ \cite{hess07}. The imaged CRs have energies between $13 $TeV and $200 $TeV and, at present, no study of the relative abundance of different cosmic ray elemental abundances exists for these energies. It could provide important clues regarding the mechanism of CR formation and propagation in the galaxy but current charge resolution from Hillas Analysis is not small enough to undertake such a study.

We consider a new method for event reconstruction, in which we fit the known Direct Cherenkov (DC) Light observed by each telescope to a characteristic Lateral Distribution Function (LDF) function. If the LDF method can achieve a resolution $ \sigma_{Z} \approx 1 $ for elements of $Z = 20$ or higher, the precision will be sufficient to extract the abundances of the different CR Elements. This the prime motivation for introducing the new LDF technique.

\section{Lateral Photon Distribution Method}
Once the Energy of a CR exceeds the local Cherenkov Energy Threshold of the atmosphere, the Nucleus will begin emitting a ring of Cherenkov Light, and continues emitting until a randomly distributed first interaction with the atmosphere. The Refractive Index of the atmosphere, and thus the Cherenkov angle $\theta_{C}$, increases as the altitude decreases. Thus the upper atmosphere emission contributes to the inner LDF, while the lower atmosphere emission contributes to the outer LDF. We find that the high-radius emission (occurring near the first interaction region) varies little between different high energies. We deem this to be \textquoteleft Saturated Emission\textquoteright.

We see that the amplitude of the LDF varies with $ \rho_{DC}  = f(r) \times Z^{2}$. Thus the amplitude of the LDF is proportional to the charge of the Cosmic Ray, enabling the Charge to be determined from the DC emission. This is the basis for charge reconstruction in the LDF method. 

In order to fully reconstruct an event, we need to find the x/y core position, the Energy per Nucleon, the first interaction height and the charge. However, if one telescope in a five-telescope array does not observe DC light, this data point can be used to constrain the core position. Thus, for the LDF method to be applied, we require a minimum of five telescopes, four or more of which must image the DC light.

We consider the amount of DC light that each telescope receives to be Poissonian and can use Stirling's Approximation to reduce computing time. We then minimise the Log Likelihood function \[ - Ln(L) = - \sum_{i=1}^{n} Ln [P_{i}] \approx  \sum_{i=1}^{n} [\lambda _{i} - N_{i} ln(\lambda _{i}) + N_{i} ln(N_{i}) - N_{i} + \frac{1}{2} ln(2 \pi N_{i})]  \]
where n is the total number of telescopes in the array.

Having reconstructed many events, we can then derive the $\sigma_{Z}$ of the dataset by assuming a Gaussian Distribution, giving us a number to directly compare the quality of event reconstruction. We find that the LDF method initially provides very poor event reconstruction because, as a result of varying Threshold Energies and the sharp drop in the LDF above the maximum radius, the Log Likelihood is frequently discontinuous. Consequently, a Minuit-type minimisation algorithm will only be able to find a local minimum near the starting values for the fit parameters. To overcome this problem, we can iterate over a series of starting values for the parameters, with the aim of scanning the true minimum among the many minima found. Minimisation typically scans 13 Z values, 10 core position coordinates, and 50 Height/Energy coordinates, yielding $ 13 \times 10 \times 50 = 6500$ minimisations in total. Such a technique is very resource intensive but typically reduces $\sigma_{Z}$ of a dataset by a factor of 5 or more.

Using one quarter of a large sample of Monte Carlo data, we can train a Boosted Decision Tree (BDT) for a given telescope multiplicity, using the reconstructed x/y core position, height and energy, as well as the Log Likelihood. The BDT is told whether each event is \textquoteleft signal' (correctly reconstructed) or \textquoteleft background' (incorrectly reconstructed). For every simulated event, this trained BDT can then be used to assign a \textquoteleft Signal Probability'. On a second quarter of the dataset we can optimise a cut on the minimum signal probability, in order to maximise the ratio of signal to background. We find that the $\sigma_{Z}$ of the remaining \textquoteleft Test' Monte Carlo data is reduced when the same BDT cut is applied.

\section{HESS-type Event Reconstruction}
With a simulation of the HESS Cherenkov 5-Telescope Array we can verify the effectiveness of the technique. For HESS cameras, the Extended Air Shower (EAS) produced after the first interaction of the CR overlaps the DC pixel and thus provides background in the LDF. As the Energy of the CR increases, the EAS speads over a larger angular area, and at smaller radii, the EAS-DC-shower direction axis contracts, leading to more overlap. Thus the background in the DC pixel increases with decreasing radius and increasing Energy. In addition, we have a fixed night sky background with 7 photons $m^{-2}$. We thus parameterise the background with an exponential in height and radius added to a fixed background. It begins to dominate above roughly 1 TeV per Nucleon, particularly in the case of smaller radii.

In the preliminary HESS simulation, it was found that the 4 telescope event reconstruction had a charge resolution of $\sigma_{Z} = 1.4$. However, requiring that the BDT signal probability satisfied $P > 0.81$ removed $75 \%$ of events, while reducing the Charge resolution to $\sigma_{Z} = 0.9$ . With this cut, core position resolution was $d \approx 1.4 m $. For 5 telescope events, it was found that the charge resolution was also $\sigma_{Z} = 1.4$. However, requiring that the signal probability satisfied $P > 0.05$ removed $53 \%$ of events, including all wrongly reconstructed ones. This placed an upper limit on the charge resolution of $\sigma_{Z} < 0.35$ . With this cut, core position resolution was $d \approx 0.8 m $.

\section{Telescope Array Optimisation}
In order to improve the count rate of high-multiplicity events, we can consider a 3x3 array of Cherenkov Telescopes, which we want to use for identifying CR Elements accurately. The \textquoteleft High Multiplicity Count Rate' of events observed by 4 or more telescopes falls with increasing grid separation. We find that the optimum grid spacing will likely lie in the 20-50m region to provide a reasonable count rate. Competing with this effect is the reliance of LDF reconstruction on sampling the entire lateral distribution. Thus the reconstruction quality will decrease as Grid Width decreases. Further study of $\sigma_{Z}$ in this region is required to determine the optimum layout (not necessarily be a grid) for event reconstruction. 

By definition, the EAS shower will arrive on the ground shortly before the DC light. There are currently several high-speed Cherenkov telescopes capable of distinguishing between these, allowing a background-free LDF to be fitted. Additional study of alternative energy regimes and layouts will also be considered for the case of a high-speed imaging telescope array. 

\section{Conclusion}
Preliminary results suggest that the LDF reconstruction technique will significantly improve charge reconstruction, to a level sufficient for cosmic ray abundance studies. However, reliance on high-multiplicity events means that although applicable to current experiments such as HESS, a new optimised telescope array would be required for a statistical analysis. Such an array may have a grid spacing of 20-50m, although further study is needed to determine the ideal layout.
\bibliographystyle{plain}
\bibliography{report}
\end{document}